{"title":"Python Scrapy爬虫框架之初次使用","slug":"Python-Scrapy","date":"2018-07-22T10:37:26.000Z","updated":"2018-07-22T11:14:40.061Z","comments":true,"path":"api/articles/Python-Scrapy.json","photos":[],"link":"","excerpt":" [Figure] 此篇博客为本人对小甲鱼的课程的总结。","covers":["/static/images/python/scrapy.png","/static/images/python/1.png","/static/images/python/2.png","/static/images/python/3.png","/static/images/python/4.png","/static/images/python/5.png","/static/images/python/6.png"],"content":"<p><img src=\"/static/images/python/scrapy.png\" alt=\"scrapy\"></p>\n<p>此篇博客为本人对<a href=\"http://www.fishc.org/\" target=\"_blank\" rel=\"noopener\">小甲鱼的课程</a>的总结。</p>\n<a id=\"more\"></a>  \n<p>关于Scrapy的安装网上都有方法，这里便不再叙述。</p>\n<p> 使用Scrapy抓取一个网站一共需要四个步骤：</p>\n<p>0、创建一个Scrapy项目；</p>\n<p>1、定义Item容器；</p>\n<p>2、编写爬虫；</p>\n<p>3、存储内容。</p>\n<p>本次爬取的目标是全球最大的目录网站<a href=\"http://www.dmoztools.net\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net</a>，由于此网站数据过于庞大，我们这里只拿它的两个子网页做测试（手动捂脸）</p>\n<p><a href=\"http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/</a></p>\n<p><a href=\"http://www.dmoztools.net/Computers/Programming/Languages/Python/Resources/\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net/Computers/Programming/Languages/Python/Resources/</a></p>\n<p>我们需要爬取的是子网页 <a href=\"http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/</a> 中的名单、超链接、名单描述。</p>\n<hr>\n<p><strong>0、创建一个Scrapy项目</strong></p>\n<p>首先打开cmd窗口输入以下命令将路径移到桌面（其他位置也可以）</p>\n<blockquote>\n<p>cd desktop</p>\n</blockquote>\n<p>然后创建一个Scrapy项目。继续在命令行中输入以下命令，（命令执行后请暂时不用关闭命令行，等下要用）</p>\n<blockquote>\n<p>scrapy startproject tutorial</p>\n</blockquote>\n<p>（创建一个新的Scrapy项目文件夹，文件夹的名字叫tutorial）文件夹中会有一个scrapy.cfg的配置文件和一个tutorial的子文件夹，tutorial文件夹中有如下内容：</p>\n<p>0、<strong>init</strong>.py  这个不用多说，是模块的初始化，不用管</p>\n<p>1、items.py  项目的容器</p>\n<p>2、pipelines.py</p>\n<p>3、settings.py 设置文件</p>\n<p>4、spiders/ 文件夹，里面只有一个初始化文件<strong>init</strong>.py ,这里需要我们来完善。</p>\n<p>等等文件（可能还有一些其他的文件，这里不多叙述）</p>\n<p><strong>1、定义Item容器</strong></p>\n<p>item容器是什么呢？item容器是保存爬取到的数据的容器，其使用方法和python字典类似，并且提供了额外的保护机制来避免拼写错误导致的未定义字段错误。</p>\n<p>接下来我们要建模。为什么要建模呢？因为item容器是用来存放我们爬取到的网页内容，当爬取的时候返回的是整个网页的内容，而往往我们只是需要其中的一部分内容比如这里的网页 <a href=\"http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/</a> ，我们只需要他的书名、描述、与超链接。</p>\n<p><img src=\"/static/images/python/1.png\" alt=\"photo\"></p>\n<p>所以这里我们需要改动item容器，打开 tutorial/item.py ，将其中的TutorialItem模块内容改为以下内容并写好注释：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TutorialItem</span><span class=\"params\">(scrapy.Item)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># define the fields for your item here like:</span></span><br><span class=\"line\">    <span class=\"comment\"># name = scrapy.Field()</span></span><br><span class=\"line\">    title = scrapy.Field()  <span class=\"comment\">#标题</span></span><br><span class=\"line\">    link = scrapy.Field()   <span class=\"comment\">#超链接</span></span><br><span class=\"line\">    desc = scrapy.Field()   <span class=\"comment\">#描述</span></span><br></pre></td></tr></table></figure>\n<p>保存退出。</p>\n<p><strong>2、编写爬虫</strong></p>\n<p>接下来是编写爬虫类Spider，Spider是用户编写用于从网站上爬取数据的类。</p>\n<p>其包含了一个用于下载的初始URL，然后是如何跟进网页中的链接以及如何分析页面中的内容，还有提取生成item的方法。</p>\n<p>接下来我们在Spider文件夹中新建一个叫 dmoz_spider.py 的文件，编写以下代码：（其中name是本次的项目名）</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DmozSpider</span><span class=\"params\">(scrapy.Spider)</span>:</span></span><br><span class=\"line\">    name = <span class=\"string\">\"dmoz\"</span></span><br><span class=\"line\">    <span class=\"comment\">#allowed_domains是为了防止在爬取的时候进入了其他网站</span></span><br><span class=\"line\">    allowed_domains = [<span class=\"string\">'dmoztools.net'</span>]</span><br><span class=\"line\">    <span class=\"comment\">#初始爬取位置</span></span><br><span class=\"line\">    star_url = [</span><br><span class=\"line\">        <span class=\"string\">'http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/'</span></span><br><span class=\"line\">        <span class=\"string\">'http://www.dmoztools.net/Computers/Programming/Languages/Python/Resources/'</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#定义一个分析方法</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse</span><span class=\"params\">(self, response)</span>:</span></span><br><span class=\"line\">        filename = response.url.split(<span class=\"string\">\"/\"</span>)[<span class=\"number\">-2</span>]</span><br><span class=\"line\">        <span class=\"keyword\">with</span> open(filename, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">            f.write(response,body)</span><br></pre></td></tr></table></figure>\n<p>保存关闭。代码解释（解释的不是很好，大概意思理解就行，不喜勿喷）：</p>\n<p>在本地目录下创建两个文件，叫做Books和Resource，然后将start_url中的两个初始网址提交给Scrapy引擎，Scrapy引擎中有下载器，会将网站的源代码下载过来，然后根据parse方法将下载的内容存放到Books和Resource中。</p>\n<p>接下来继续进入刚才的cmd窗口，输入以下命令将路径切换到tutorial文件夹中：</p>\n<blockquote>\n<p>cd tutorial</p>\n</blockquote>\n<p>继续在命令行输入以下命令：</p>\n<blockquote>\n<p>scrapy crawl dmoz</p>\n</blockquote>\n<p>其中crawl是爬取的意思，dmoz是项目名。</p>\n<p>然后输入回车执行。成功的话将会出现以下信息：</p>\n<p><img src=\"/static/images/python/2.png\" alt=\"photo\"></p>\n<p>并且命令执行后在当前目录会生成两个新的文件，叫做Books和Resource。两个文件的内容就是爬取到的网站内容（即网站的源代码）。</p>\n<p><img src=\"/static/images/python/3.png\" alt=\"photo\"></p>\n<p><strong>3、存储内容</strong></p>\n<p>以往我们在爬取网页内容的时候都是使用的html的正则表达式，但是在Scrapy中是使用一种基于Xpath和CSS的表达式机制：Scrapy Selectors</p>\n<p>Selectors是一个选择器，它有四个基本的方法：</p>\n<p>xpath()： 传入spath表达式，返回该表达式所对应的所有节点的selector list列表</p>\n<p>css()：传入CSS表达式，返回该表达式所对应的所有节点的selector list列表</p>\n<p>extract()：序列化该节点为unicode字符串并返回list</p>\n<p>re()：根据传入的正则表达式对数据进行提取，返回unicode字符串list列表</p>\n<p>存储数据之前我们先进去scrapy的shell窗口进行测试。继续对刚才的cmd窗口进行操作，输入以下命令进入shell窗口：</p>\n<blockquote>\n<p>scrapy shell “<a href=\"http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/&quot;\" target=\"_blank\" rel=\"noopener\">http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/&quot;</a></p>\n</blockquote>\n<p>目的是进入该网站的scrapy shell窗口，界面如下：</p>\n<p><img src=\"/static/images/python/4.png\" alt=\"photo\"></p>\n<p>此时我们便可以对他进行操作。</p>\n<p>其实这是返回我们刚才的response对象，我们可以对他进行一系列操作，比如response.body命令将会出现该网站的源代码，response.headers命令会出现网站的头配置等等</p>\n<p> 现在我们列举几个该对象的使用方法：</p>\n<p>/html/head/title：选择html文档中<head>标签内的<title>元素；</title></head></p>\n<p>/html/head/title/text()：选择上面提到的<title>元素的文字；</title></p>\n<p>//td：选择所有的<td>元素；</td></p>\n<p>//div[@class=’mine’]：选择所有具有class=’mine’属性的div元素。</p>\n<p>此时shell窗口中还有 一个返回对象sel，接下来我们进行查找名单的测试。</p>\n<p>输入命令：（以下出现的路径请自行检查网页内容进行查看，右键点击网页对象选择检查即可出现）</p>\n<blockquote>\n<p>sel.xpath(‘//section/div/div/div/div/a/div/text()’).extract()</p>\n</blockquote>\n<p>执行后将会返回网页中的所有名单，图如下：</p>\n<p><img src=\"/static/images/python/5.png\" alt=\"photo\"></p>\n<p>同样，输入命令以下命令将会返回所有名单的超链接和描述</p>\n<blockquote>\n<p>sel.xpath(‘//section/div/div/div/div/a/@href’).extract() 　　//返回超链接<br>sel.xpath(‘//section/div/div/div/div/a/div/text()’).extract()　　　　//返回名单描述</p>\n</blockquote>\n<p>好，测试完了之后我们来继续编写代码。</p>\n<p>现在我们来对爬取内容进行筛选</p>\n<p>打开编辑我们刚才编写的 dmoz_spider.py 文件，将内容改为以下内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> tutorial.items <span class=\"keyword\">import</span> TutorialItem <span class=\"comment\">#引入模块</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">DmozSpider</span><span class=\"params\">(scrapy.Spider)</span>:</span></span><br><span class=\"line\">    name = <span class=\"string\">\"dmoz\"</span></span><br><span class=\"line\">    <span class=\"comment\">#allowed_domains是为了防止在爬取的时候进入了其他网站</span></span><br><span class=\"line\">    allowed_domains = [<span class=\"string\">'dmoztools.net'</span>]</span><br><span class=\"line\">    <span class=\"comment\">#初始爬取位置</span></span><br><span class=\"line\">    start_urls = [</span><br><span class=\"line\">        <span class=\"string\">'http://www.dmoztools.net/Computers/Programming/Languages/Python/Books/'</span>,</span><br><span class=\"line\">        <span class=\"string\">'http://www.dmoztools.net/Computers/Programming/Languages/Python/Resources/'</span></span><br><span class=\"line\">        ]</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">#定义一个分析方法</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parse</span><span class=\"params\">(self, response)</span>:</span></span><br><span class=\"line\">        sel = scrapy.selector.Selector(response) <span class=\"comment\">#分析返回的response对象</span></span><br><span class=\"line\">        <span class=\"comment\">#//section/div/div/div/div/a/div/text()</span></span><br><span class=\"line\">        sites = sel.xpath(<span class=\"string\">'//section/div/div/div/div'</span>)    <span class=\"comment\">#对内容进行筛选</span></span><br><span class=\"line\">        items = []    </span><br><span class=\"line\">        <span class=\"keyword\">for</span> site <span class=\"keyword\">in</span> sites:    <span class=\"comment\">#对爬取内容进行进一步筛选</span></span><br><span class=\"line\">            item = TutorialItem()</span><br><span class=\"line\">            item[<span class=\"string\">'title'</span>] = site.xpath(<span class=\"string\">'a/div/text()'</span>).extract()</span><br><span class=\"line\">            item[<span class=\"string\">'link'</span>] = site.xpath(<span class=\"string\">'a/@href'</span>).extract()</span><br><span class=\"line\">            item[<span class=\"string\">'desc'</span>] = site.xpath(<span class=\"string\">'div/text()'</span>).extract()</span><br><span class=\"line\">            items.append(item)    <span class=\"comment\">#将筛选后的内容添加到列表items中</span></span><br><span class=\"line\">            </span><br><span class=\"line\">        <span class=\"keyword\">return</span> items</span><br></pre></td></tr></table></figure>\n<p>保存退出。然后继续在cmd窗口进行操作，首先退出shell窗口：</p>\n<blockquote>\n<p>exit()  </p>\n</blockquote>\n<p>随后开始爬取网站内容并将筛选后的数据导出，进行以下命令：</p>\n<blockquote>\n<p>scrapy crawl dmoz -o items.json -t json</p>\n</blockquote>\n<p>-o后面接导出的文件名；-t后面接导出的形式，形式常用的有四种：json、xml、jsonlines以及csv，这里我使用的是json。</p>\n<p>执行后在当前路径会产生一个 items.json 的文件，我们使用文本编辑器打开它时会发现，里面就是我们本次要爬取的筛选后的内容：</p>\n<p><img src=\"/static/images/python/6.png\" alt=\"photo\"></p>\n<p>程序到这里便成功完成！</p>\n<p> 谢谢观看！(๑•̀ㅂ•́)و✧</p>\n<blockquote>\n<p><em>我演了很多悲剧，到头来你们都说那是喜剧。            ——————周星驰</em></p>\n</blockquote>\n","categories":[{"name":"Python","slug":"Python","count":2,"path":"api/categories/Python.json"}],"tags":[{"name":"Python","slug":"Python","count":2,"path":"api/tags/Python.json"}]}